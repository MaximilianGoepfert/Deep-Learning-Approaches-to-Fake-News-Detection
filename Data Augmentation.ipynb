{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, CSVLogger\n",
    "from  tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from  tensorflow.keras.preprocessing import sequence\n",
    "from  tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from  tensorflow.keras.models import Sequential\n",
    "from  tensorflow.keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout,Bidirectional\n",
    "from  tensorflow.keras.layers import Embedding\n",
    "from  tensorflow.keras import optimizers\n",
    "from  tensorflow.keras.layers import Input\n",
    "from  tensorflow.keras.models import Model\n",
    "from  tensorflow.keras.utils import plot_model\n",
    "from  tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "# Read in dataset and adjust it for two class setting\n",
    "\n",
    "#####\n",
    "liar_df_train = pd.read_csv('liar_dataset/train.tsv',sep='\\t',header=None)\n",
    "liar_df_val = pd.read_csv('liar_dataset/valid.tsv',sep='\\t',header=None)\n",
    "liar_df_test = pd.read_csv('liar_dataset/test.tsv',sep='\\t',header=None)\n",
    "columns = ['ID','label','text','subjects','speaker','job','state','party','barely_true_count',\n",
    "           'false_count','half_true_count','mostly_true_count','pants_on_fire_count','location']\n",
    "liar_df_train.columns = columns\n",
    "liar_df_val.columns = columns\n",
    "liar_df_test.columns = columns\n",
    "\n",
    "liar_df_train = liar_df_train.drop(labels=['ID'],axis=1)\n",
    "liar_df_test = liar_df_test.drop(labels=['ID'],axis=1)\n",
    "liar_df_val = liar_df_val.drop(labels=['ID'],axis=1)\n",
    "liar_list = [liar_df_train,liar_df_val,liar_df_test]\n",
    "#truth=  {\"pants-fire\" : 0, \"false\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"true\" : 5}\n",
    "truth=  {\"pants-fire\" : 0, \"false\" : 0, \"barely-true\" : 0, \"half-true\" : 1, \"mostly-true\" : 1, \"true\" : 1}\n",
    "\n",
    "for i in liar_list:\n",
    "    i['numer_truth'] = i['label'].apply(lambda x: truth[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_df_train['numer_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "# Define functions for data augmentation\n",
    "\n",
    "####\n",
    "\n",
    "import names\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "def change_person(x):\n",
    "    \n",
    "    x=nlp(x)\n",
    "    person_names = [ent.text for ent in x.doc.ents if ent.label_ == \"PERSON\"]\n",
    "    if person_names:\n",
    "        name_to_replace = np.random.choice(person_names)\n",
    "        replacement_name = np.random.choice(replacement_names)\n",
    "        return x.text.replace(name_to_replace, replacement_name)\n",
    "\n",
    "\n",
    "def swap_adjectives(x):\n",
    "    x=nlp(x)\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if len(adjective_idxs) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_idxs, 2, replace=False))\n",
    "        return \" \".join(\n",
    "            [\n",
    "                x.doc[:idx1].text,\n",
    "                x.doc[idx2].text,\n",
    "                x.doc[1 + idx1 : idx2].text,\n",
    "                x.doc[idx1].text,\n",
    "                x.doc[1 + idx2 :].text,\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def replace_adjective_with_synonym(x):\n",
    "    # Get indices of adjective tokens in sentence.\n",
    "    x=nlp(x)\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adjective_idxs:\n",
    "        # Pick random adjective idx to replace.\n",
    "        idx = np.random.choice(adjective_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "        # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            #x.text = replace_token(x.doc, idx, synonym)\n",
    "            return replace_token(x.doc, idx, synonym)\n",
    "\n",
    "        \n",
    "##### Replace noun function eventually not used in this project\n",
    "\n",
    "#def replace_noun_with_synonym(x):\n",
    "#    # Get indices of noun tokens in sentence.\n",
    "#    x=nlp(x)\n",
    "#    noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "#    if noun_idxs:\n",
    "#        # Pick random noun idx to replace.\n",
    "#        idx = np.random.choice(noun_idxs)\n",
    "#        synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "#        # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "#        if synonym:\n",
    "#            #x.text = replace_token(x.doc, idx, synonym)\n",
    "#            return replace_token(x.doc, idx, synonym)\n",
    "\n",
    "def replace_verb_with_synonym(x):\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    x=nlp(x)\n",
    "    verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    if verb_idxs:\n",
    "        # Pick random verb idx to replace.\n",
    "        idx = np.random.choice(verb_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "        # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            #x.text = replace_token(x.doc, idx, synonym)\n",
    "            return replace_token(x.doc, idx, synonym)\n",
    "\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    return \" \".join([spacy_doc[:idx].text, replacement, spacy_doc[1 + idx :].text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply data augmentation techniques to training set\n",
    "\n",
    "liar_df_train['adjectives']=liar_df_train['text'].apply(swap_adjectives)\n",
    "liar_df_train['change_person']=liar_df_train['text'].apply(change_person)\n",
    "liar_df_train['replace_verb_with_synonym']=liar_df_train['text'].apply(replace_verb_with_synonym)\n",
    "liar_df_train['replace_adjective_with_synonym']=liar_df_train['text'].apply(replace_adjective_with_synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save augmented data in temporary files\n",
    "\n",
    "temp=liar_df_train.copy()\n",
    "\n",
    "temp1=temp[temp['adjectives'].notna()][['adjectives','numer_truth']].reset_index().drop('index',axis=1).rename(\n",
    "    columns={'adjectives':'text'})\n",
    "\n",
    "\n",
    "temp2 =temp[temp['change_person'].notna()][['change_person','numer_truth']].reset_index().drop('index',axis=1).rename(\n",
    "    columns={'change_person':'text'})\n",
    "\n",
    "\n",
    "temp3 = temp[temp['replace_verb_with_synonym'].notna()][['replace_verb_with_synonym','numer_truth']].reset_index().drop('index',axis=1).rename(\n",
    "    columns={'replace_verb_with_synonym':'text'})\n",
    "\n",
    "\n",
    "temp4 = temp[temp['replace_adjective_with_synonym'].notna()][['replace_adjective_with_synonym','numer_truth']].reset_index().drop('index',axis=1).rename(\n",
    "    columns={'replace_adjective_with_synonym':'text'})\n",
    "\n",
    "len(temp1)+len(temp2)+len(temp3)+len(temp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge original training set and augmented data from temp files\n",
    "\n",
    "temp = pd.concat([temp1, temp2,temp3,temp4])\n",
    "\n",
    "liar_df_train = pd.concat([temp,liar_df_train])\n",
    "\n",
    "liar_df_train.numer_truth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenize training data and create dictionary\n",
    "\n",
    "def load_statement_vocab_dict(train_data):\n",
    "    vocabulary_dict = {}  \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data['text'])\n",
    "    vocabulary_dict = tokenizer.word_index\n",
    "    print(len(vocabulary_dict))\n",
    "    return vocabulary_dict\n",
    "\n",
    "vocabulary_dict = load_statement_vocab_dict(liar_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define function to remove stopwords and prepare text sequences\n",
    "\n",
    "def preprocess_statement(statement):\n",
    "    from nltk.corpus import stopwords\n",
    "    statement = [w for w in statement.split(' ') if w not in stopwords.words('english')]\n",
    "    statement = ' '.join(statement)\n",
    "    text = text_to_word_sequence(statement)  \n",
    "    val = [0] * 10\n",
    "    val = [vocabulary_dict[t] for t in text if t in vocabulary_dict] \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply preprocessing function to all statements\n",
    "\n",
    "liar_df_train['word_id'] = liar_df_train['text'].apply(preprocess_statement)\n",
    "liar_df_val['word_id'] = liar_df_val['text'].apply(preprocess_statement)\n",
    "liar_df_test['word_id'] = liar_df_test['text'].apply(preprocess_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save resulting datasets in pickle files\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(liar_df_val,open('augmemted_val.pkl','wb'))\n",
    "pickle.dump(liar_df_test,open('augmemted_test.pkl','wb'))\n",
    "pickle.dump(liar_df_train,open('augmemted_train2.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
